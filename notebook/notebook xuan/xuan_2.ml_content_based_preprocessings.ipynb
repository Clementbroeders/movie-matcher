{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORT & READ CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "content = pd.read_csv('./src/TMDB_content.csv')\n",
    "content = content.drop(['poster_path', 'year', 'watch_providers'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PREPROCESSINGS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. KEYWORDS\n",
    "1. Remove occurence < 1\n",
    "\n",
    "2. Transform keyword to its lemma form\n",
    "\n",
    "3. Lowercase and remove spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = content.copy()\n",
    "df['keywords'] = df['keywords'].str.split(',')\n",
    "df = df.explode('keywords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "keywords\n",
       " based on novel or book    332\n",
       " duringcreditsstinger      240\n",
       " murder                    202\n",
       " sequel                    196\n",
       " california                171\n",
       "                          ... \n",
       " minefield                   1\n",
       " corsican                    1\n",
       " twilight                    1\n",
       " streetwise                  1\n",
       " free fall                   1\n",
       "Name: count, Length: 10589, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = df['keywords'].value_counts()\n",
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "keywords\n",
       " based on novel or book    332\n",
       " duringcreditsstinger      240\n",
       " murder                    202\n",
       " sequel                    196\n",
       " california                171\n",
       "                          ... \n",
       " around the world            2\n",
       " canoe                       2\n",
       " candle                      2\n",
       "secret identity              2\n",
       " covid-19                    2\n",
       "Name: count, Length: 4995, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = k[k>1]\n",
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_keywords(x):\n",
    "    ''' Remove keywords that appear only one time '''\n",
    "    words = []\n",
    "    for i in x:\n",
    "        if i in k:\n",
    "            words.append(i)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def to_lemma(text):\n",
    "    ''' Transform keyword into its lemma form '''\n",
    "    doc = nlp(text)\n",
    "    lemmatized_tokens = [token.lemma_ for token in doc]\n",
    "    lemmatized_text = ' '.join(lemmatized_tokens)\n",
    "    return lemmatized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "content['keywords'] = content['keywords'].apply(lambda x: str(x).split(','))\n",
    "content['keywords'] = content['keywords'].apply(filter_keywords)\n",
    "content['keywords'] = content['keywords'].apply(lambda x: [to_lemma(i) for i in x])\n",
    "content['keywords'] = content['keywords'].apply(lambda x: [str.lower(i.replace(' ','')) for i in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "keyword\n",
       "baseonnovelorbook       496\n",
       "duringcreditsstinger    244\n",
       "sequel                  209\n",
       "murder                  202\n",
       "newyorkcity             201\n",
       "                       ... \n",
       "universe                  2\n",
       "earth                     2\n",
       "warstrategy               2\n",
       "slowmotion                2\n",
       "platoniclove              2\n",
       "Name: count, Length: 4649, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_clean = content.apply(lambda x: pd.Series(x['keywords']),axis=1).stack().reset_index(level=1, drop=True)\n",
    "k_clean.name = 'keyword'\n",
    "k_clean.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RÃ©sults : \n",
    "1. keywords are replaced by its lemma (ex. 'base' VS 'based on novel')\n",
    "\n",
    "2. keywords that appear only once are removed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. GENRES, CAST, DIRECTOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "content['genres'] = content['genres'].apply(lambda x: str(x).split(','))\n",
    "content['genres'] = content['genres'].apply(lambda x: [str.lower(i.replace(' ','')) for i in x])\n",
    "\n",
    "content['cast'] = content['cast'].apply(lambda x: str(x).split(','))\n",
    "content['cast'] = content['cast'].apply(lambda x: [str.lower(i.replace(' ','')) for i in x])\n",
    "\n",
    "content['director'] = content['director'].apply(lambda x: [x,x,x])\n",
    "content['director'] = content['director'].apply(lambda x: [str.lower(i.replace(' ','')) for i in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We keep 3 actors as the main cast:\n",
    "if len(content['cast']) > 3:\n",
    "    content['main_cast'] = content['cast'].apply(lambda x: x[0:3])\n",
    "else:\n",
    "    content['main_cast'] = content['cast']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GATHER AND SAVE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "content['soup'] = content['genres'] + content['keywords'] + content['main_cast'] + content['director']\n",
    "content['soup'] = content['soup'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_content = content.drop(['title', 'genres', 'keywords', 'cast', 'director', 'main_cast'], axis=1)\n",
    "preprocessed_content.to_csv('./src/preprocessed_content_3_3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "moviematch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
